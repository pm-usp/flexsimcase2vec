# -*- coding: utf-8 -*-
"""AnomalyDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fu3MpBTdjhBOio4vWrqQ3PTCLHoAB9rl

#Install libraries if executing in Google Colab
"""


"""#Import libraries"""

import os
import pickle
import pandas as pd
import numpy as np
import plotly.express as px

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import average_precision_score
from sklearn.metrics import confusion_matrix

from sklearn.cluster import DBSCAN
from sklearn.svm import OneClassSVM

import pm4py
from pm4py.objects.log.exporter.xes import exporter as xes_exporter
from pm4py.objects.conversion.log import converter as log_converter
from pm4py.objects.log.util import dataframe_utils


import sys
sys.path.append('../representation/')

from node2vec4cases.Node2vec4cases import Node2vec4cases
from flexsimcase2vec.FlexSimCase2vec import FlexSimCase2vec
from case2vec.replearn.eventlog import EventLog
from case2vec.replearn.doc2vec import Doc2VecRepresentation

from gbrltvPMencoding.encode.graph import run_node2vec as Node2vec_DFG


"""#Methods"""

def get_log_id_anomaly_type(log_file):
  log_id = log_file.replace('.csv', '')
  anomaly_type = log_file.split('_')[3].replace('at','')
  return log_id, anomaly_type

def one_hot(log, repres_model_id, use_variant, cases_variants_df = None):
  if os.path.isfile(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv'):
    return pd.read_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv')
  if use_variant:
    if cases_variants_df is None:
      raise Exception('Variant id information is required!')
    representation = pd.get_dummies(cases_variants_df[[CASE_ID_COL, 'variant_id']], columns=['variant_id'], dtype=int)
  else:
    representation = (pd.get_dummies(log[[CASE_ID_COL, ACTIVITY_COL]], columns=[ACTIVITY_COL])
                      .groupby(CASE_ID_COL).sum().astype(bool).astype(int).reset_index())
  representation.to_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv', index=False)
  return representation

def one_hot_original(log, repres_model_id, use_variant, cases_variants_df = None):
    file_path = f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv'
    if os.path.isfile(file_path):
        return pd.read_csv(file_path)

    activity_one_hot = pd.get_dummies(log[ACTIVITY_COL])
    log_encoded = log[[CASE_ID_COL]].join(activity_one_hot)

    representation_df = log_encoded.groupby(CASE_ID_COL).apply(lambda x: x.drop(columns=[CASE_ID_COL]).to_numpy().flatten())
    representation_df = representation_df.apply(pd.Series).fillna(0).rename_axis(CASE_ID_COL).reset_index()

    representation_df.to_csv(file_path, index=False)
    return representation_df

def trace2vec(log, repres_model_id, use_variant, cases_variants_df = None):
  if os.path.isfile(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv'):
    return pd.read_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv')

  log_filename = repres_model_id.split('-')[0]
  event_log = EventLog(log_filename, true_cluster_label='concept:name')
  event_log.load(get_xes(log, log_filename), False)
  event_log.preprocess()
  doc2vec = Doc2VecRepresentation(event_log)
  doc2vec.build_model(append_case_attr=False, append_event_attr=False, vector_size=EMBEDDINGS_DIMENSIONS, concat=True, epochs=50)
  doc2vec.fit()
  feature_vector = doc2vec.predict(epochs=50)
  vectors = pd.DataFrame(feature_vector)
  vectors[CASE_ID_COL] = doc2vec._event_log.case_ids
  vectors[CASE_ID_COL] = vectors[CASE_ID_COL].astype(log[CASE_ID_COL].dtype)
  vectors.to_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv', index=False)
  return vectors

def node2vec_DFG(log, repres_model_id, use_variant, cases_variants_df = None):
  if os.path.isfile(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv'):
    return pd.read_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv')
  log_filename = repres_model_id.split('-')[0]
  logxes = pm4py.read_xes(get_xes(log, log_filename))
  logxes["case:concept:name"] = logxes["case:concept:name"].astype("string")
  logxes["concept:name"] = logxes["concept:name"].astype("string")
  config = {"vector_size": EMBEDDINGS_DIMENSIONS, "aggregation":"average", "embed_from":"nodes", "edge_operator":"average"}
  vectors = Node2vec_DFG(config, logxes)
  vectors = vectors.rename(columns={"case":CASE_ID_COL}) #'case' is the name of the column the authors gave to the resulting df
  vectors[CASE_ID_COL] = vectors[CASE_ID_COL].astype(log[CASE_ID_COL].dtype)
  vectors.to_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv', index=False)
  return vectors

def node2vec_case(log, repres_model_id, use_variant, cases_variants_df = None):
  log_df = log.copy()
  if use_variant:
    log_df = log.merge(cases_variants_df, on=CASE_ID_COL, how='left')
  
  n2v4c = Node2vec4cases(log_df=log_df, log_id=repres_model_id.split("-")[0], case_col_key=CASE_ID_COL,
                         activity_col_key=ACTIVITY_COL, variant_nodes=use_variant, variant_col_key='variant_id',
                         resulting_graph_dir=f'{RESULTS_DIR_PATH}node2vec4cases-graphs/', 
                         resultind_embeddings_dir=f'{RESULTS_DIR_PATH}representations/',
                         dimensions=EMBEDDINGS_DIMENSIONS, seed=SEED, verbose=False)

  _, vectors = n2v4c.fit()
  return vectors

def flexsimcase2vec(log, repres_model_id, use_variant, cases_variants_df = None):
  def get_embeddings(df, model):
    traces_ids = df[CASE_ID_COL].unique()
    embeddings = [list(model.wv.get_vector(c)) for c in traces_ids]
    embeddings = pd.DataFrame(embeddings)
    embeddings[CASE_ID_COL] = pd.Series(traces_ids)
    return embeddings

  if os.path.isfile(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv'):
    return pd.read_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv')
  df = log
  if use_variant:
    if cases_variants_df is None:
      raise Exception('Variant id information is required!')
    df = cases_variants_df
    fsc2v = FlexSimCase2vec(cases_variants_df,
                            trace_key=CASE_ID_COL, sim_cols=['variant_id'],
                            seed=SEED, dimensions=EMBEDDINGS_DIMENSIONS)

  else:
    fsc2v = FlexSimCase2vec(log,
                            trace_key=CASE_ID_COL, sim_cols=[ACTIVITY_COL],
                            seed=SEED, dimensions=EMBEDDINGS_DIMENSIONS)
  model = fsc2v.fit()
  vectors = get_embeddings(df, model)
  vectors.to_csv(f'{RESULTS_DIR_PATH}representations/{repres_model_id}.csv', index=False)
  return vectors

def get_variant_ids(log):
  df = log.groupby(CASE_ID_COL)[ACTIVITY_COL].apply(lambda x: '_'.join(x)).reset_index().rename(columns={ACTIVITY_COL:'variant'})
  le = LabelEncoder()
  df['variant_id'] = le.fit_transform(df['variant'])
  return df[[CASE_ID_COL, 'variant_id']].drop_duplicates()

def get_xes(log, log_filename):
  xes_filepath = f"{LOGS_DIR_PATH}{log_filename}.xes"
  if not os.path.isfile(xes_filepath):
    log4xes = log.copy()
    #create a fake timestamp for this Node2vec implementation to run as it requires a time date column
    log4xes["time:timestamp"] = "1970-01-01 00:00:00"
    log4xes["time:timestamp"] = pd.to_datetime(log4xes["time:timestamp"])
    log4xes = log4xes.rename(columns={ACTIVITY_COL: "concept:name", CASE_ID_COL: "case:concept:name"})
    event_log = log_converter.apply(log4xes)
    xes_exporter.apply(event_log, xes_filepath)
  return xes_filepath

def add_label(df, log):
  df[CASE_ID_COL] = df[CASE_ID_COL].astype(log[CASE_ID_COL].dtype)
  return pd.merge(df, log[[CASE_ID_COL, ANOMALY_COL]].drop_duplicates(), on=CASE_ID_COL, how='left')

def predict_and_evaluate(model_id, X_train, X_test, y_train, y_test):
  #model = LogisticRegression(max_iter=100000)
  if not os.path.isfile(f'{RESULTS_DIR_PATH}predictive_models/{model_id}.model'):
    model = RandomForestClassifier()
    model.fit(X_train.values, y_train.values)
    pickle.dump(model, open(f'{RESULTS_DIR_PATH}predictive_models/{model_id}.model', 'wb'))
  else:
    model = pickle.load(open(f'{RESULTS_DIR_PATH}predictive_models/{model_id}.model', 'rb'))
  y_pred = model.predict(X_test)
  y_test_numeric = y_test.map({'normal': 0, 'anomaly': 1}).values
  y_pred_numeric = [1 if x == 'anomaly' else 0 for x in y_pred]
  y_train_pred = model.predict(X_train)
  y_train_numeric = y_train.map({'normal': 0, 'anomaly': 1}).values
  y_train_pred_numeric = [1 if x == 'anomaly' else 0 for x in y_train_pred]
  return average_precision_score(y_train_numeric, y_train_pred_numeric, pos_label=1), average_precision_score(y_test_numeric, y_pred_numeric, pos_label=1)

def cluster_and_evaluate(model_id, vectors, alg='dbscan'):
    from sklearn.cluster import DBSCAN
    from sklearn.svm import OneClassSVM
    from sklearn.metrics import average_precision_score

    def evaluate(y_true, y_scores, y_labels):
        ap_score = average_precision_score(y_true, y_scores)
        ap_binary = average_precision_score(y_true, y_labels)
        return ap_score, ap_binary

    file_path = f'{RESULTS_DIR_PATH}clustering_results/{model_id}.csv'

    X = vectors.drop(columns=[CASE_ID_COL, ANOMALY_COL]).values
    y_true = vectors[ANOMALY_COL].map({'normal': 0, 'anomaly': 1}).values

    if alg == 'ocsvm':
        model = OneClassSVM(kernel='linear')
        model.fit(X)
        scores = -model.decision_function(X)
        binary_preds = np.where(model.predict(X) == -1, 1, 0)

    elif alg == 'dbscan':
        clustering = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit_predict(X)
        scores = (clustering == -1).astype(int)  # binary only
        binary_preds = scores  # mesma coisa aqui

    else:
        raise ValueError(f"Unsupported method: {alg}")

    result_df = pd.DataFrame({
        CASE_ID_COL: vectors[CASE_ID_COL],
        'anomaly_score': scores,
        'anomaly_label': binary_preds
    })
    result_df.to_csv(file_path, index=False)

    return evaluate(y_true, scores, binary_preds)



"""# Define global variables"""

LOGS_DIR_PATH = "data/" #'/content/drive/MyDrive/ProcessMiningResearch/Trace_Representation/experiments/anomaly_detection/data/'
RESULTS_DIR_PATH = "results/"#'/content/drive/MyDrive/ProcessMiningResearch/Trace_Representation/experiments/'

CASE_ID_COL = 'traceid'
ACTIVITY_COL = 'activity'
ANOMALY_COL = 'label'

REPRESENTATIONS = [one_hot, one_hot_original, node2vec_case, flexsimcase2vec, trace2vec, node2vec_DFG]
EMBEDDINGS_DIMENSIONS = 8
USE_VARIANT = [True]#[False, True]
SEED = 42

PREDICTIVE_MODEL = False
MODEL = LogisticRegression #RandomForestClassifier
MODEL_ALIAS = 'lg' #'rf'
UNSUPERVISED_MODEL_TYPE = 'ocsvm'  # or 'dbscan'

EPS = 0.5
MIN_SAMPLES = 10

"""#Load data"""

logs = [l for l in os.listdir(LOGS_DIR_PATH) if '.csv' in l]

"""#Create vector representations, train models and evaluate results"""

results = {}
for i, log_file in enumerate(logs):
  #print(f'\n-------\nLog {i+1}\n-------\n')
  log = pd.read_csv(os.path.join(LOGS_DIR_PATH, log_file))
  log_id, anomaly_type = get_log_id_anomaly_type(log_file)
  case_variants = None
  for use_variant in USE_VARIANT:
    if USE_VARIANT:
      cases_variants = get_variant_ids(log)
    for representation in REPRESENTATIONS:
      repres_model_id = f'{log_id}-{representation.__qualname__}-variants{use_variant}'
      if (representation.__qualname__ == 'trace2vec' or
          representation.__qualname__ == 'node2vec_DFG' or
          representation.__qualname__ == 'one_hot_original') and not use_variant:
          continue
      vectors = representation(log, repres_model_id, use_variant, cases_variants_df = cases_variants)
      vectors = add_label(vectors, log)
      if PREDICTIVE_MODEL:
        train_test_sets = train_test_split(vectors.drop(columns=[CASE_ID_COL, ANOMALY_COL]), vectors[ANOMALY_COL], test_size=0.2, random_state=42)
        results[f'{repres_model_id}-{MODEL_ALIAS}'] = [anomaly_type, representation.__qualname__, use_variant, *predict_and_evaluate(f'{repres_model_id}-{MODEL_ALIAS}', *train_test_sets)]
      else:
        model_id = f'{repres_model_id}-{UNSUPERVISED_MODEL_TYPE.upper()}'
        ap_score, ap_binary = cluster_and_evaluate(model_id, vectors, alg=UNSUPERVISED_MODEL_TYPE)
        results[model_id] = [anomaly_type, representation.__qualname__, use_variant, ap_score, ap_binary]


results_id = RESULTS_DIR_PATH

if PREDICTIVE_MODEL:

  results_id += MODEL_ALIAS
  df_results = (pd.DataFrame.from_dict(results, orient='index', columns=['anomaly_type', 'representation', 'variant', 'AP_sobrep', 'AP'])
                            .reset_index()
                            .rename(columns={'index': 'log'}))

  df_results['repres_id'] = df_results['representation'] + '-variant_' + df_results['variant'].astype(str)
  df_results.sort_values(by=['anomaly_type','representation','variant'])

  fig = px.box(df_results, x='anomaly_type', y='AP', color='anomaly_type', facet_col='repres_id', 
    points='all', template='plotly_white', width=500*len(df_results.representation.unique()), height=400)
  fig.update_yaxes(range = [0,1])
  print(f'Resulting graph saved: {results_id}.png')
  fig.write_image(f'{results_id}.png')
  fig.show()

  #fig = px.box(df_results, x='anomaly_type', y='AP_sobrep', color='anomaly_type', facet_col='representation', facet_row='variant',
  #           points='all', template='plotly_white', width=400*len(df_results.representation.unique()), height=600)
  #fig.update_yaxes(range = [0,1])
  #fig.show()

else:
  results_id += f'OCSVM'
  df_results = (pd.DataFrame.from_dict(results, orient='index', columns=['anomaly_type', 'representation', 'variant', 'AP_score', 'AP_binary'])
              .reset_index()
              .rename(columns={'index': 'log'}))

  df_results['repres_id'] = df_results['representation'] + '-variant_' + df_results['variant'].astype(str)
  df_results.sort_values(by=['anomaly_type','representation','variant'])

  fig = px.box(df_results, x='anomaly_type', y='AP_score', color='anomaly_type', facet_col='repres_id',
               points='all', template='plotly_white', width=500*len(df_results.representation.unique()), height=400)
  fig.update_yaxes(range = [0,1])
  fig.write_image(f'{results_id}--score_eval.png')
  fig.write_html(f'{results_id}--score_eval.html')
  print(f'Resulting graph saved: {results_id}--score.png')

  fig = px.box(df_results, x='anomaly_type', y='AP_binary', color='anomaly_type', facet_col='repres_id',
               points='all', template='plotly_white', width=500*len(df_results.representation.unique()), height=400)
  fig.update_yaxes(range = [0,1])
  fig.write_image(f'{results_id}--binary_eval.png')
  fig.write_html(f'{results_id}--binary_eval.html')
  print(f'Resulting graph saved: {results_id}--binary_eval.png')

df_results.to_csv(f'{results_id}-results_table.csv', index=False)

